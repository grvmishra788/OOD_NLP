{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "import sklearn.metrics as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename='./data/r8-train.txt'):\n",
    "    '''\n",
    "    :param filename: the system location of the data to load\n",
    "    :return: the text (x) and its label (y)\n",
    "             the text is a list of words and is not processed\n",
    "    '''\n",
    "\n",
    "    # stop words taken from nltk\n",
    "    stop_words = ['i','me','my','myself','we','our','ours','ourselves','you','your','yours',\n",
    "                  'yourself','yourselves','he','him','his','himself','she','her','hers','herself',\n",
    "                  'it','its','itself','they','them','their','theirs','themselves','what','which',\n",
    "                  'who','whom','this','that','these','those','am','is','are','was','were','be',\n",
    "                  'been','being','have','has','had','having','do','does','did','doing','a','an',\n",
    "                  'the','and','but','if','or','because','as','until','while','of','at','by','for',\n",
    "                  'with','about','against','between','into','through','during','before','after',\n",
    "                  'above','below','to','from','up','down','in','out','on','off','over','under',\n",
    "                  'again','further','then','once','here','there','when','where','why','how','all',\n",
    "                  'any','both','each','few','more','most','other','some','such','no','nor','not',\n",
    "                  'only','own','same','so','than','too','very','s','t','can','will','just','don',\n",
    "                  'should','now','d','ll','m','o','re','ve','y','ain','aren','couldn','didn',\n",
    "                  'doesn','hadn','hasn','haven','isn','ma','mightn','mustn','needn','shan',\n",
    "                  'shouldn','wasn','weren','won','wouldn']\n",
    "\n",
    "    x, y = [], []\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = re.sub(r'\\W+', ' ', line).strip()\n",
    "            x.append(line[1:])\n",
    "            x[-1] = ' '.join(word for word in x[-1].split() if word not in stop_words)\n",
    "            y.append(line[0])\n",
    "    return x, np.array(y, dtype=int)\n",
    "\n",
    "def get_vocab(dataset):\n",
    "    '''\n",
    "    :param dataset: the text from load_data\n",
    "\n",
    "    :return: a _ordered_ dictionary from words to counts\n",
    "    '''\n",
    "    vocab = {}\n",
    "\n",
    "    # create a counter for each word\n",
    "    for example in dataset:\n",
    "        example_as_list = example.split()\n",
    "        for word in example_as_list:\n",
    "            vocab[word] = 0\n",
    "\n",
    "    for example in dataset:\n",
    "        example_as_list = example.split()\n",
    "        for word in example_as_list:\n",
    "            vocab[word] += 1\n",
    "    \n",
    "    # sort from greatest to least by count\n",
    "    return collections.OrderedDict(sorted(vocab.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "def text_to_rank(dataset, _vocab, desired_vocab_size=1000):\n",
    "    '''\n",
    "    :param dataset: the text from load_data\n",
    "    :vocab: a _ordered_ dictionary of vocab words and counts from get_vocab\n",
    "    :param desired_vocab_size: the desired vocabulary size\n",
    "    words no longer in vocab become UUUNNNKKK\n",
    "    :return: the text corpus with words mapped to their vocab rank,\n",
    "    with all sufficiently infrequent words mapped to UUUNNNKKK; UUUNNNKKK has rank desired_vocab_size\n",
    "    (the infrequent word cutoff is determined by desired_vocab size)\n",
    "    '''\n",
    "    _dataset = dataset[:]     # aliasing safeguard\n",
    "    vocab_ordered = list(_vocab)\n",
    "    count_cutoff = _vocab[vocab_ordered[desired_vocab_size-2]] # get word by its rank and map to its count\n",
    "    \n",
    "    word_to_rank = {}\n",
    "    for i in range(len(vocab_ordered)):\n",
    "        # we add one to make room for any future padding symbol with value 0\n",
    "        word_to_rank[vocab_ordered[i]] = i\n",
    "    \n",
    "    for i in range(len(_dataset)):\n",
    "        example = _dataset[i]\n",
    "        example_as_list = example.split()\n",
    "        for j in range(len(example_as_list)):\n",
    "            try:\n",
    "                if _vocab[example_as_list[j]] >= count_cutoff and word_to_rank[example_as_list[j]] < desired_vocab_size:\n",
    "                    # we need to ensure that other words below the word on the edge of our desired_vocab size\n",
    "                    # are not also on the count cutoff\n",
    "                    example_as_list[j] = word_to_rank[example_as_list[j]] \n",
    "                else:\n",
    "                    example_as_list[j] = desired_vocab_size-1  # UUUNNNKKK\n",
    "            except:\n",
    "                example_as_list[j] = desired_vocab_size-1  # UUUNNNKKK\n",
    "        _dataset[i] = example_as_list\n",
    "\n",
    "    return _dataset\n",
    "\n",
    "def text_to_matrix(dataset, _vocab, desired_vocab_size=1000):\n",
    "    sequences = text_to_rank(dataset, _vocab, desired_vocab_size)\n",
    "    \n",
    "    mat = np.zeros((len(sequences), desired_vocab_size), dtype=int)\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        for token in seq:\n",
    "            mat[i][token] = 1\n",
    "    \n",
    "    return mat\n",
    "\n",
    "def get_vocab(dataset):\n",
    "    '''\n",
    "    :param dataset: the text from load_data\n",
    "\n",
    "    :return: a _ordered_ dictionary from words to counts\n",
    "    '''\n",
    "    vocab = {}\n",
    "\n",
    "    # create a counter for each word\n",
    "    for example in dataset:\n",
    "        example_as_list = example.split()\n",
    "        for word in example_as_list:\n",
    "            vocab[word] = 0\n",
    "\n",
    "    for example in dataset:\n",
    "        example_as_list = example.split()\n",
    "        for word in example_as_list:\n",
    "            vocab[word] += 1\n",
    "\n",
    "    # sort from greatest to least by count\n",
    "    return collections.OrderedDict(sorted(vocab.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partion_data_in_two(dataset, dataset_labels, in_sample_labels, oos_labels):\n",
    "    '''\n",
    "    :param dataset: the text from text_to_rank\n",
    "    :param dataset_labels: dataset labels\n",
    "    :param in_sample_labels: a list of newsgroups which the network will/did train on\n",
    "    :param oos_labels: the complement of in_sample_labels; these newsgroups the network has never seen\n",
    "    :return: the dataset partitioned into in_sample_examples, in_sample_labels,\n",
    "    oos_examples, and oos_labels in that order\n",
    "    '''\n",
    "    _dataset = dataset[:]     # aliasing safeguard\n",
    "    _dataset_labels = dataset_labels\n",
    "    \n",
    "    in_sample_idxs = np.zeros(np.shape(_dataset_labels), dtype=bool)\n",
    "    ones_vec = np.ones(np.shape(_dataset_labels), dtype=int)\n",
    "    for label in in_sample_labels:\n",
    "        in_sample_idxs = np.logical_or(in_sample_idxs, _dataset_labels == label * ones_vec)\n",
    "\n",
    "    \n",
    "    return _dataset[in_sample_idxs], _dataset_labels[in_sample_idxs],\\\n",
    "        _dataset[np.logical_not(in_sample_idxs)], _dataset_labels[np.logical_not(in_sample_idxs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our network trains only on a subset of classes, say 6, but class number 7 might still\n",
    "# be an in-sample label: we need to squish the labels to be in {0,...,5}\n",
    "def relabel_in_sample_labels(labels):\n",
    "    labels_as_list = labels.tolist()\n",
    "    \n",
    "    set_of_labels = []\n",
    "    for label in labels_as_list:\n",
    "        set_of_labels.append(label)\n",
    "    labels_ordered = sorted(list(set(set_of_labels)))\n",
    "    \n",
    "    relabeled = np.zeros(labels.shape, dtype=int)\n",
    "    for i in range(len(labels_as_list)):\n",
    "        relabeled[i] = labels_ordered.index(labels_as_list[i])\n",
    "    \n",
    "    return relabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "vocab_size = 1000\n",
    "num_epochs = 5\n",
    "n_hidden = 512\n",
    "nclasses_to_exclude = 2  # 0-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_classes = np.arange(8)\n",
    "np.random.shuffle(random_classes)\n",
    "to_include = list(random_classes[:8-nclasses_to_exclude])\n",
    "to_exclude = list(random_classes[8-nclasses_to_exclude:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data\n",
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "print('Loading Data')\n",
    "X_train, Y_train = load_data()\n",
    "X_test, Y_test = load_data('./data/r8-test.txt')\n",
    "\n",
    "vocab = get_vocab(X_train)\n",
    "X_train = text_to_matrix(X_train, vocab, vocab_size)\n",
    "X_test = text_to_matrix(X_test, vocab, vocab_size)\n",
    "\n",
    "# shuffle\n",
    "indices = np.arange(X_train.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X_train = X_train[indices]\n",
    "Y_train = Y_train[indices]\n",
    "\n",
    "indices = np.arange(X_test.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X_test = X_test[indices]\n",
    "Y_test = Y_test[indices]\n",
    "\n",
    "# split into train/dev\n",
    "X_dev = X_train[-500:]\n",
    "Y_dev = Y_train[-500:]\n",
    "X_train = X_train[:-500]\n",
    "Y_train = Y_train[:-500]\n",
    "\n",
    "in_sample_examples, in_sample_labels, oos_examples, oos_labels =\\\n",
    "partion_data_in_two(X_train, Y_train, to_include, to_exclude)\n",
    "dev_in_sample_examples, dev_in_sample_labels, dev_oos_examples, dev_oos_labels =\\\n",
    "partion_data_in_two(X_dev, Y_dev, to_include, to_exclude)\n",
    "test_in_sample_examples, test_in_sample_labels, test_oos_examples, dev_oos_labels =\\\n",
    "partion_data_in_two(X_test, Y_test, to_include, to_exclude)\n",
    "\n",
    "# safely assumes there is an example for each in_sample class in both the training and dev class \n",
    "in_sample_labels = relabel_in_sample_labels(in_sample_labels)\n",
    "dev_in_sample_labels = relabel_in_sample_labels(dev_in_sample_labels)\n",
    "test_in_sample_labels = relabel_in_sample_labels(test_in_sample_labels)\n",
    "\n",
    "num_examples = in_sample_labels.shape[0]\n",
    "num_batches = num_examples//batch_size\n",
    "\n",
    "print('Data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Applications/anaconda3/envs/pytorch/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Applications/anaconda3/envs/pytorch/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    x = tf.placeholder(dtype=tf.float32, shape=[None, vocab_size])\n",
    "    y = tf.placeholder(dtype=tf.int64, shape=[None])\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "    # add one to vocab size for the padding symbol\n",
    "\n",
    "    W_h = tf.Variable(tf.nn.l2_normalize(tf.random_normal([vocab_size, n_hidden]), 0)/tf.sqrt(1 + 0.45))\n",
    "    b_h = tf.Variable(tf.zeros([n_hidden]))\n",
    "    \n",
    "    def gelu_fast(_x):\n",
    "        return 0.5 * _x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (_x + 0.044715 * tf.pow(_x, 3))))\n",
    "    \n",
    "    h = tf.cond(is_training,\n",
    "                lambda: tf.nn.dropout(gelu_fast(tf.matmul(x, W_h) + b_h), 0.5),\n",
    "                lambda: gelu_fast(tf.matmul(x, W_h) + b_h))\n",
    "    \n",
    "    W_out = tf.Variable(tf.nn.l2_normalize(tf.random_normal([n_hidden, 8-nclasses_to_exclude]), 0)/tf.sqrt(0.45 + 1))\n",
    "    b_out = tf.Variable(tf.zeros([8-nclasses_to_exclude]))\n",
    "    \n",
    "    logits = tf.matmul(h, W_out) + b_out\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    lr = tf.train.exponential_decay(1e-3, global_step, 4*num_batches, 0.1, staircase=True)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss, global_step=global_step)\n",
    "\n",
    "    acc = 100*tf.reduce_mean(tf.to_float(tf.equal(tf.argmax(logits, 1), y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Applications/anaconda3/envs/pytorch/lib/python3.8/site-packages/tensorflow/python/util/tf_should_use.py:243: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-07 15:31:22.544059: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "sess = tf.InteractiveSession(graph=graph)\n",
    "tf.initialize_all_variables().run()\n",
    "# create saver to train model\n",
    "saver = tf.train.Saver(max_to_keep=1)\n",
    "\n",
    "print('Initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Minibatch loss 0.879 | Minibatch accuracy 78.125 | Dev accuracy 83.654\n",
      "Epoch 2 | Minibatch loss 0.590 | Minibatch accuracy 87.500 | Dev accuracy 90.385\n",
      "Epoch 3 | Minibatch loss 0.255 | Minibatch accuracy 93.750 | Dev accuracy 96.154\n",
      "Epoch 4 | Minibatch loss 0.234 | Minibatch accuracy 100.000 | Dev accuracy 95.192\n",
      "Epoch 5 | Minibatch loss 0.074 | Minibatch accuracy 100.000 | Dev accuracy 96.154\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # shuffle data every epoch\n",
    "    indices = np.arange(num_examples)\n",
    "    np.random.shuffle(indices)\n",
    "    in_sample_examples = in_sample_examples[indices]\n",
    "    in_sample_labels = in_sample_labels[indices]\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        offset = i * batch_size\n",
    "\n",
    "        x_batch = in_sample_examples[offset:offset + batch_size]\n",
    "        y_batch = in_sample_labels[offset:offset + batch_size]\n",
    "\n",
    "        _, l, batch_acc = sess.run([optimizer, loss, acc], feed_dict={x: x_batch, y: y_batch, is_training: True})\n",
    "\n",
    "\n",
    "    curr_dev_acc = sess.run(\n",
    "        acc, feed_dict={x: dev_in_sample_examples, y: dev_in_sample_labels, is_training: False})\n",
    "    if best_acc < curr_dev_acc:\n",
    "        best_acc = curr_dev_acc\n",
    "        saver.save(sess, './data/best_r8_model.ckpt')\n",
    "\n",
    "    print('Epoch %d | Minibatch loss %.3f | Minibatch accuracy %.3f | Dev accuracy %.3f' %\n",
    "          (epoch+1, l, batch_acc, curr_dev_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./data/best_r8_model.ckpt\n",
      "Best model restored!\n",
      "Dev accuracy: 96.15385\n"
     ]
    }
   ],
   "source": [
    "# restore variables from disk\n",
    "saver.restore(sess, \"./data/best_r8_model.ckpt\")\n",
    "print(\"Best model restored!\")\n",
    "\n",
    "print('Dev accuracy:', sess.run(acc, feed_dict={x: dev_in_sample_examples, y: dev_in_sample_labels, is_training:False}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Applications/anaconda3/envs/pytorch/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /Applications/anaconda3/envs/pytorch/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1082: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "s = tf.nn.softmax(logits)\n",
    "s_prob = tf.reduce_max(s, reduction_indices=[1], keep_dims=True)\n",
    "kl_all = tf.log(8. - nclasses_to_exclude)\\\n",
    "        + tf.reduce_sum(s * tf.log(tf.abs(s) + 1e-10), reduction_indices=[1], keep_dims=True)\n",
    "m_all, v_all = tf.nn.moments(kl_all, axes=[0])\n",
    "\n",
    "logits_right = tf.boolean_mask(logits, tf.equal(tf.argmax(logits, 1), y))\n",
    "s_right = tf.nn.softmax(logits_right)\n",
    "s_right_prob = tf.reduce_max(s_right, reduction_indices=[1], keep_dims=True)\n",
    "kl_right = tf.log(8. - nclasses_to_exclude)\\\n",
    "         + tf.reduce_sum(s_right * tf.log(tf.abs(s_right) + 1e-10), reduction_indices=[1], keep_dims=True)\n",
    "m_right, v_right = tf.nn.moments(kl_right, axes=[0])\n",
    "\n",
    "logits_wrong = tf.boolean_mask(logits, tf.not_equal(tf.argmax(logits, 1), y))\n",
    "s_wrong = tf.nn.softmax(logits_wrong)\n",
    "s_wrong_prob = tf.reduce_max(s_wrong, reduction_indices=[1], keep_dims=True)\n",
    "kl_wrong = tf.log(8. - nclasses_to_exclude)\\\n",
    "           + tf.reduce_sum(s_wrong * tf.log(tf.abs(s_wrong) + 1e-10), reduction_indices=[1], keep_dims=True)\n",
    "m_wrong, v_wrong = tf.nn.moments(kl_wrong, axes=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reuters8 (w/class subset) Error (%)| Prediction Prob (mean, std) | PProb Right (mean, std) | PProb Wrong (mean, std):\n",
      "10.731712 | 0.7804771 0.2260942 | 0.8106825 0.20919667 | 0.5292236 0.20392188\n",
      "\n",
      "Success Detection\n",
      "Success base rate (%): 89.27\n",
      "KL[p||u]: Right/Wrong classification distinction\n",
      "AUPR (%): 96.81\n",
      "AUROC (%): 78.46\n",
      "Prediction Prob: Right/Wrong classification distinction\n",
      "AUPR (%): 97.43\n",
      "AUROC (%): 82.51\n",
      "\n",
      "Error Detection\n",
      "Error base rate (%): 10.73\n",
      "KL[p||u]: Right/Wrong classification distinction\n",
      "AUPR (%): 28.11\n",
      "AUROC (%): 78.46\n",
      "Prediction Prob: Right/Wrong classification distinction\n",
      "AUPR (%): 33.8\n",
      "AUROC (%): 82.51\n"
     ]
    }
   ],
   "source": [
    "err, kl_a, kl_r, kl_w, s_p, s_rp, s_wp = sess.run(\n",
    "    [100 - acc, kl_all, kl_right, kl_wrong, s_prob, s_right_prob, s_wrong_prob],\n",
    "    feed_dict={x: test_in_sample_examples, y: test_in_sample_labels, is_training: False})\n",
    "\n",
    "print('Reuters8 (w/class subset) Error (%)| Prediction Prob (mean, std) | PProb Right (mean, std) | PProb Wrong (mean, std):')\n",
    "print(err, '|', np.mean(s_p), np.std(s_p), '|', np.mean(s_rp), np.std(s_rp), '|', np.mean(s_wp), np.std(s_wp))\n",
    "\n",
    "print('\\nSuccess Detection')\n",
    "print('Success base rate (%):', round(100-err,2))\n",
    "print('KL[p||u]: Right/Wrong classification distinction')\n",
    "safe, risky = kl_r, kl_w\n",
    "labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "labels[:safe.shape[0]] += 1\n",
    "examples = np.squeeze(np.vstack((safe, risky)))\n",
    "print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "print('Prediction Prob: Right/Wrong classification distinction')\n",
    "safe, risky = s_rp, s_wp\n",
    "labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "labels[:safe.shape[0]] += 1\n",
    "examples = np.squeeze(np.vstack((safe, risky)))\n",
    "print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "\n",
    "print('\\nError Detection')\n",
    "print('Error base rate (%):', round(err,2))\n",
    "safe, risky = -kl_r, -kl_w\n",
    "labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "labels[safe.shape[0]:] += 1\n",
    "examples = np.squeeze(np.vstack((safe, risky)))\n",
    "print('KL[p||u]: Right/Wrong classification distinction')\n",
    "print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "print('Prediction Prob: Right/Wrong classification distinction')\n",
    "safe, risky = -s_rp, -s_wp\n",
    "labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "labels[safe.shape[0]:] += 1\n",
    "examples = np.squeeze(np.vstack((safe, risky)))\n",
    "print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_ood_detection_results(error_rate_for_in, in_examples, out_examples):\n",
    "    kl_oos, s_p_oos = sess.run([kl_all, s_prob], feed_dict={x: out_examples, is_training: False})\n",
    "\n",
    "    print('OOD Example Prediction Probability (mean, std):')\n",
    "    print(np.mean(s_p_oos), np.std(s_p_oos))\n",
    "\n",
    "    print('\\nNormality Detection')\n",
    "    print('Normality base rate (%):', round(100*in_examples.shape[0]/(\n",
    "                out_examples.shape[0] + in_examples.shape[0]),2))\n",
    "    print('KL[p||u]: Normality Detection')\n",
    "    safe, risky = kl_a, kl_oos\n",
    "    labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "    labels[:safe.shape[0]] += 1\n",
    "    examples = np.squeeze(np.vstack((safe, risky)))\n",
    "    print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "    print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "    print('Prediction Prob: Normality Detection')\n",
    "    safe, risky = s_p, s_p_oos\n",
    "    labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "    labels[:safe.shape[0]] += 1\n",
    "    examples = np.squeeze(np.vstack((safe, risky)))\n",
    "    print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "    print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "    print('Normality base rate (%):', round(100*(1 - err/100)*in_examples.shape[0]/\n",
    "          (out_examples.shape[0] + (1 - err/100)*in_examples.shape[0]),2))\n",
    "    print('KL[p||u]: Normality Detection (relative to correct examples)')\n",
    "    safe, risky = kl_r, kl_oos\n",
    "    labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "    labels[:safe.shape[0]] += 1\n",
    "    examples = np.squeeze(np.vstack((safe, risky)))\n",
    "    print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "    print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "    print('Prediction Prob: Normality Detection (relative to correct examples)')\n",
    "    safe, risky = s_rp, s_p_oos\n",
    "    labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "    labels[:safe.shape[0]] += 1\n",
    "    examples = np.squeeze(np.vstack((safe, risky)))\n",
    "    print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "    print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "\n",
    "    print('\\n\\nAbnormality Detection')\n",
    "    print('Abnormality base rate (%):', round(100*out_examples.shape[0]/(\n",
    "                out_examples.shape[0] + in_examples.shape[0]),2))\n",
    "    print('KL[p||u]: Abnormality Detection')\n",
    "    safe, risky = -kl_a, -kl_oos\n",
    "    labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "    labels[safe.shape[0]:] += 1\n",
    "    examples = np.squeeze(np.vstack((safe, risky)))\n",
    "    print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "    print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "    print('Prediction Prob: Normality Detection')\n",
    "    safe, risky = -s_p, -s_p_oos\n",
    "    labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "    labels[safe.shape[0]:] += 1\n",
    "    examples = np.squeeze(np.vstack((safe, risky)))\n",
    "    print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "    print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "    print('Abnormality base rate (%):', round(100*out_examples.shape[0]/\n",
    "          (out_examples.shape[0] + (1 - err/100)*in_examples.shape[0]),2))\n",
    "    print('KL[p||u]: Normality Detection (relative to correct examples)')\n",
    "    safe, risky = -kl_r, -kl_oos\n",
    "    labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "    labels[safe.shape[0]:] += 1\n",
    "    examples = np.squeeze(np.vstack((safe, risky)))\n",
    "    print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "    print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "    print('Prediction Prob: Normality Detection (relative to correct examples)')\n",
    "    safe, risky = -s_rp, -s_p_oos\n",
    "    labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "    labels[safe.shape[0]:] += 1\n",
    "    examples = np.squeeze(np.vstack((safe, risky)))\n",
    "    print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "    print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Held-out subjects\n",
      "\n",
      "OOD Example Prediction Probability (mean, std):\n",
      "0.41601706 0.15130463\n",
      "\n",
      "Normality Detection\n",
      "Normality base rate (%): 18.73\n",
      "KL[p||u]: Normality Detection\n",
      "AUPR (%): 76.8\n",
      "AUROC (%): 89.94\n",
      "Prediction Prob: Normality Detection\n",
      "AUPR (%): 75.4\n",
      "AUROC (%): 88.51\n",
      "Normality base rate (%): 17.06\n",
      "KL[p||u]: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 78.59\n",
      "AUROC (%): 91.55\n",
      "Prediction Prob: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 78.08\n",
      "AUROC (%): 91.02\n",
      "\n",
      "\n",
      "Abnormality Detection\n",
      "Abnormality base rate (%): 81.27\n",
      "KL[p||u]: Abnormality Detection\n",
      "AUPR (%): 96.39\n",
      "AUROC (%): 89.94\n",
      "Prediction Prob: Normality Detection\n",
      "AUPR (%): 95.71\n",
      "AUROC (%): 88.51\n",
      "Abnormality base rate (%): 82.94\n",
      "KL[p||u]: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 97.34\n",
      "AUROC (%): 91.55\n",
      "Prediction Prob: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 97.1\n",
      "AUROC (%): 91.02\n"
     ]
    }
   ],
   "source": [
    "print('Held-out subjects\\n')\n",
    "show_ood_detection_results(err, test_in_sample_examples, test_oos_examples)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
